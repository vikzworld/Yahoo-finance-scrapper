{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant libraries\n",
    "\n",
    "import requests \n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "# enter the path of the csv file to store the list of tickers for which you need\n",
    "# market cap, dividend yield and % off from 52 week high details\n",
    "security_list = pd.read_csv('C:\\\\Users\\\\shauvik\\\\Desktop\\\\Google Drive Backup\\\\Dividend\\\\Input\\\\Ex-div Security List.csv')\n",
    "\n",
    "######################## Fetch market cap from Yahoo Finance ########################\n",
    "\n",
    "# setup the base url4\n",
    "ticker_url2 = \"https://finance.yahoo.com/quote/\"\n",
    "\n",
    "# create an empty list to capture and aggregate the details for each ticker\n",
    "marketcap_details = []\n",
    "\n",
    "# loop through the csv file \n",
    "for row2 in security_list.itertuples(index=True):\n",
    "    a2 = (getattr(row2,\"Ticker\"))\n",
    "    \n",
    "    # construct the final url for each ticker\n",
    "    market_cap_url = ticker_url2 + str(a2)\n",
    "    \n",
    "    # read the content of each url using Beautiful Soup\n",
    "    details = requests.get(market_cap_url,allow_redirects=False)\n",
    "    details_status = details.status_code\n",
    "    c2 = details.content\n",
    "    \n",
    "    soup2 = BeautifulSoup(c2,\"html.parser\")\n",
    "    \n",
    "    # check for a valid Yahoo Finance url for each ticker\n",
    "    if details_status == 200:\n",
    "        \n",
    "        # inspect the Yahoo Finance page to figure out the exact html tag structure\n",
    "        page_validity_check1 = soup2.find('td', class_='Ta(end) Fw(600) Lh(14px)')\n",
    "        \n",
    "        if page_validity_check1 == 'None':\n",
    "            print(a2, \" does not have market cap details.\")              \n",
    "            \n",
    "        # only capture details from valid urls   \n",
    "        else:\n",
    "            market_cap = soup2.find_all('td', class_='Ta(end) Fw(600) Lh(14px)')[8].text\n",
    "            \n",
    "            # create an empty dictionary to create a easy reference of market cap figure for each ticker\n",
    "            market_cap_list = {}\n",
    "            \n",
    "            # use try and except framework to check for errors\n",
    "            try:   \n",
    "                for mktcap in market_cap:\n",
    "                    if mktcap == 'N/A':\n",
    "                        continue\n",
    "                \n",
    "                    market_cap_list['Ticker'] = str(a2)\n",
    "                    market_cap_list['Market Cap Details'] = market_cap\n",
    "                    \n",
    "            except IndexError:\n",
    "                print(a2, \" does not have market cap details.\")\n",
    "            \n",
    "            # update the empty master list with the dictionary data\n",
    "            marketcap_details.append(market_cap_list)\n",
    "        \n",
    "    else:\n",
    "        print(a2, \" is not a valid url.\")\n",
    "\n",
    "# convert the master list to a pandas dataframe        \n",
    "df_marketcap = pd.DataFrame(marketcap_details)\n",
    "\n",
    "# Determine the units of market cap figure        \n",
    "df_marketcap['Market Cap Unit'] = df_marketcap['Market Cap Details'].str.get(-1) \n",
    "\n",
    "# Only extract the numbers from the string\n",
    "df_marketcap['Market Cap'] = df_marketcap['Market Cap Details'].str.extract('(\\d+\\.\\d*)') \n",
    "\n",
    "# Convert string to numeric (i.e. float data type) and convert all market cap figures to Millions\n",
    "df_marketcap['Market Cap'] = df_marketcap['Market Cap'].astype(float).fillna(0.0)                   \n",
    "df_marketcap['Market Cap (Mn)'] = df_marketcap.apply(lambda row: (row['Market Cap']/1000 \n",
    "                                               if row['Market Cap Unit']=='M'\n",
    "                                               else row['Market Cap']),axis=1)                                \n",
    "\n",
    "df_marketcap = df_marketcap[['Ticker','Market Cap (Mn)']]\n",
    "\n",
    "######################## Fetch the latest dividend yield of tickers from Yahoo Fiannce ########################\n",
    "\n",
    "# setup a base url\n",
    "url_temp = \"https://finance.yahoo.com/quote/\"\n",
    "\n",
    "yld = []\n",
    "\n",
    "# lopp through the list of tickers from the df_marketcap dataframe created previously\n",
    "for row in df_marketcap.itertuples(index=True):\n",
    "    a1 = (getattr(row,\"Ticker\"))\n",
    "    \n",
    "    # construct the actual url for each ticker\n",
    "    url = url_temp + str(a1)\n",
    "    \n",
    "    # read the content of each url using Beautiful Soup \n",
    "    details = requests.get(url)\n",
    "    s = details.status_code\n",
    "    c2 = details.content\n",
    "    \n",
    "    soup2 = BeautifulSoup(c2,\"html.parser\")\n",
    "    \n",
    "    # check for valid Yahoo Finance pages\n",
    "    if s == 200:\n",
    "\n",
    "        check = soup2.find_all(\"td\", class_='Ta(end) Fw(600) Lh(14px)')\n",
    "        \n",
    "        if check == 'None':\n",
    "            print(a1, \" does not have yield details.\")  \n",
    "            \n",
    "        else:\n",
    "            a2 = soup2.find_all(\"td\", class_='Ta(end) Fw(600) Lh(14px)')[13].text\n",
    "            \n",
    "            yld_list = {}\n",
    "            \n",
    "            try:   \n",
    "                for i in a2:\n",
    "                    if i == 'N/A':\n",
    "                        continue\n",
    "                \n",
    "                    yld_list['Ticker'] = str(a1)\n",
    "                    yld_list['Yield Details'] = a2\n",
    "                    \n",
    "            except IndexError:\n",
    "                print(a1, \" does not have yield details.\")\n",
    "            \n",
    "            yld.append(yld_list)\n",
    "        \n",
    "    else:\n",
    "        print(a1, \" is not a valid url.\")\n",
    "\n",
    "# convert the list to a pandas dataframe\n",
    "df_yld = pd.DataFrame(yld)   \n",
    "\n",
    "# use pandas dataframe string methods to split and extarct text between parenthesis\n",
    "# also make sure to exclude % sign from yield to retain numerical format for easy calculations\n",
    "df_yld['Yield'] = df_yld['Yield Details'].str.split('(').str[1].str.split('%').str[0]\n",
    "df_yld = df_yld[['Ticker','Yield']]\n",
    "\n",
    "# merge the 2 dataframes across a common index, i.e. ticker\n",
    "df_merge1 = df_marketcap.set_index('Ticker').join(df_yld.set_index('Ticker'))\n",
    "\n",
    "######################## Fetch the 52 week high of tickers from Yahoo Finance ########################\n",
    "\n",
    "# again setup a base url\n",
    "ticker_url3 = \"https://finance.yahoo.com/quote/\"\n",
    "\n",
    "other_details = []\n",
    "\n",
    "# loop through the tickers from the first dataframe\n",
    "# rest of the procedure follows a similar pattern like above\n",
    "for row3 in df_marketcap.itertuples(index=True):\n",
    "    a3 = (getattr(row3,\"Ticker\"))\n",
    "    other_details_url = ticker_url3 + str(a3)\n",
    "    other_det = requests.get(other_details_url)\n",
    "    other_details_status = other_det.status_code\n",
    "    c3 = other_det.content\n",
    "    \n",
    "    soup3 = BeautifulSoup(c3,\"html.parser\")\n",
    "        \n",
    "    if other_details_status == 200:\n",
    "        page_validity_check2 = soup3.find_all(\"td\", class_='Ta(end) Fw(600) Lh(14px)')\n",
    "        \n",
    "        if page_validity_check2 == []:\n",
    "            print(a3, \" does not have 52 week details.\")\n",
    "    \n",
    "        else:\n",
    "            other1 = soup3.find_all(\"td\", class_='Ta(end) Fw(600) Lh(14px)')[5]\n",
    "    \n",
    "            other_list = {}\n",
    "        \n",
    "            try:   \n",
    "                other_list['Ticker'] = str(a3)\n",
    "                other_list['52 week range'] = other1.text\n",
    "                \n",
    "            except IndexError:\n",
    "                print(a3, \" does not have 52 week details.\")\n",
    "            \n",
    "            other_details.append(other_list)\n",
    "        \n",
    "    else:\n",
    "        print(a3, \" does not have 52 week details.\")\n",
    "                \n",
    "df_other_details = pd.DataFrame(other_details)\n",
    "\n",
    "# extarct the 2nd part of the string appearing after '-'\n",
    "df_other_details['52 Week High'] = df_other_details['52 week range'].str.split('-').str[1] \n",
    "\n",
    "# convert the column to a numeric format\n",
    "df_other_details['52 Week High'] = df_other_details['52 Week High'].astype(float)\n",
    "df_other_details = df_other_details[['Ticker','52 Week High']]\n",
    "\n",
    "df_merge2 = df_other_details.set_index('Ticker').join(df_merge1)\n",
    "\n",
    "######################## Fetch the latest closing price data from Yahoo Finance ########################\n",
    "\n",
    "# setup a base url\n",
    "# rest of the procedure is similar as above\n",
    "ticker_url4 = \"https://finance.yahoo.com/quote/\"\n",
    "\n",
    "closing_price_hist = []\n",
    "for row4 in df_marketcap.itertuples(index=True):\n",
    "    a4 = (getattr(row4,\"Ticker\"))\n",
    "    closing_price_url = ticker_url4 + str(a4) + '/history'\n",
    "    close = requests.get(closing_price_url)\n",
    "    close_status = close.status_code\n",
    "    c4 = close.content\n",
    "    \n",
    "    soup4 = BeautifulSoup(c4,\"html.parser\")\n",
    "\n",
    "    if close_status == 200:\n",
    "        page_validity_check3 = soup4.find_all('td', class_='Py(10px) Pstart(10px)')\n",
    "               \n",
    "        if page_validity_check3 == 'None':\n",
    "            print(a4, \" does not have closing price details.\")\n",
    "            \n",
    "        else:\n",
    "            closing_price = soup4.find_all('td', class_='Py(10px) Pstart(10px)')[4]\n",
    "            closing_date = soup4.find_all('td', class_='Py(10px) Ta(start) Pend(10px)')[0]\n",
    "    \n",
    "            closing_price_list = {}\n",
    "        \n",
    "            try:   \n",
    "                closing_price_list['Ticker'] = str(a4)\n",
    "                closing_price_list['Closing Price'] = closing_price.text\n",
    "                closing_price_list['Closing Date'] = closing_date.text\n",
    "               \n",
    "            except IndexError:\n",
    "                print(a4, \" does not have closing price details.\")\n",
    "            \n",
    "            closing_price_hist.append(closing_price_list)\n",
    "        \n",
    "    else:\n",
    "        print(a4, \" does not have closing price details.\")\n",
    "        \n",
    "df_closing_price = pd.DataFrame(closing_price_hist)\n",
    "\n",
    "# convert the column to a numeric format\n",
    "df_closing_price['Closing Price'] = df_closing_price['Closing Price'].astype(float)\n",
    "df_closing_price = df_closing_price[['Ticker','Closing Price','Closing Date']]\n",
    "\n",
    "######################## Merge everythinng ########################\n",
    "\n",
    "df_merge_all = df_closing_price.set_index('Ticker').join(df_merge2)\n",
    "\n",
    "# calculate the % off from 52 week high and store it in a new dataframe column 'Off from 52-week High'\n",
    "df_merge_all['Off from 52-week High'] = df_merge_all.apply(lambda row: (((row['Closing Price']-row['52 Week High'])/row['52 Week High'])*100), axis=1)\n",
    "\n",
    "df_merge_all = df_merge_all[['Market Cap (Mn)','Yield','Off from 52-week High']]\n",
    "\n",
    "df_merge_all.to_csv(\"C:\\\\Users\\\\shauvik\\\\Desktop\\\\Google Drive Backup\\\\Dividend\\\\Ex-div output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
